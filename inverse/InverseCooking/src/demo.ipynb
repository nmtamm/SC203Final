{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverse Cooking: Recipe Generation from Food Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "from args import get_parser\n",
    "import pickle\n",
    "from model import get_model\n",
    "from torchvision import transforms\n",
    "from utils.output_utils import prepare_output\n",
    "from PIL import Image\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set ```data_dir``` to the path including vocabularies and model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code will run in gpu if available and if the flag is set to True, else it will run on cpu\n",
    "use_gpu = False\n",
    "device = torch.device('cuda' if torch.cuda.is_available() and use_gpu else 'cpu')\n",
    "map_loc = None if torch.cuda.is_available() and use_gpu else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code below was used to save vocab files so that they can be loaded without Vocabulary class\n",
    "#ingrs_vocab = pickle.load(open(os.path.join(data_dir, 'final_recipe1m_vocab_ingrs.pkl'), 'rb'))\n",
    "#ingrs_vocab = [min(w, key=len) if not isinstance(w, str) else w for w in ingrs_vocab.idx2word.values()]\n",
    "#vocab = pickle.load(open(os.path.join(data_dir, 'final_recipe1m_vocab_toks.pkl'), 'rb')).idx2word\n",
    "#pickle.dump(ingrs_vocab, open('../demo/ingr_vocab.pkl', 'wb'))\n",
    "#pickle.dump(vocab, open('../demo/instr_vocab.pkl', 'wb'))\n",
    "\n",
    "ingrs_vocab = pickle.load(open(os.path.join(data_dir, 'ingr_vocab.pkl'), 'rb'))\n",
    "vocab = pickle.load(open(os.path.join(data_dir, 'instr_vocab.pkl'), 'rb'))\n",
    "\n",
    "ingr_vocab_size = len(ingrs_vocab)\n",
    "instrs_vocab_size = len(vocab)\n",
    "output_dim = instrs_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (instrs_vocab_size, ingr_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "import sys; sys.argv=['']; del sys\n",
    "args = get_parser()\n",
    "args.maxseqlen = 15\n",
    "args.ingrs_only=False\n",
    "model = get_model(args, ingr_vocab_size, instrs_vocab_size)\n",
    "# Load the trained model parameters\n",
    "model_path = os.path.join(data_dir, 'modelbest.ckpt')\n",
    "model.load_state_dict(torch.load(model_path, map_location=map_loc))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "model.ingrs_only = False\n",
    "model.recipe_only = False\n",
    "print ('loaded model')\n",
    "print (\"Elapsed time:\", time.time() -t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transf_list_batch = []\n",
    "transf_list_batch.append(transforms.ToTensor())\n",
    "transf_list_batch.append(transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                                              (0.229, 0.224, 0.225)))\n",
    "to_input_transf = transforms.Compose(transf_list_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy = [True, False, False, False]\n",
    "beam = [-1, -1, -1, -1]\n",
    "temperature = 1.0\n",
    "numgens = len(greedy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set ```use_urls = True``` to get recipes for images in ```demo_urls```. \n",
    "\n",
    "You can also set ```use_urls = False``` and get recipes for images in the path in ```data_dir/test_imgs```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from io import BytesIO\n",
    "import random\n",
    "from collections import Counter\n",
    "use_urls = False # set to true to load images from demo_urls instead of those in test_imgs folder\n",
    "show_anyways = False #if True, it will show the recipe even if it's not valid\n",
    "image_folder = os.path.join(data_dir, 'demo_imgs')\n",
    "\n",
    "# Ver 4: Remove random to get a fix set of images\n",
    "if not use_urls:\n",
    "    demo_imgs = os.listdir(image_folder)\n",
    "    # random.shuffle(demo_imgs)\n",
    "\n",
    "demo_urls = ['https://food.fnr.sndimg.com/content/dam/images/food/fullset/2013/12/9/0/FNK_Cheesecake_s4x3.jpg.rend.hgtvcom.826.620.suffix/1387411272847.jpeg',\n",
    "            'https://www.196flavors.com/wp-content/uploads/2014/10/california-roll-3-FP.jpg']\n",
    "\n",
    "demo_files = demo_urls if use_urls else demo_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils.metrics import softIoU\n",
    "from utils.metrics import update_error_types, compute_metrics\n",
    "\n",
    "# Function to convert label indices to one-hot encoded vectors\n",
    "def label2onehot(labels, pad_value):\n",
    "    inp_ = torch.unsqueeze(labels, 2)\n",
    "    one_hot = (\n",
    "        torch.FloatTensor(labels.size(0), labels.size(1), pad_value + 1)\n",
    "        .zero_()\n",
    "        .to(device)\n",
    "    )\n",
    "    one_hot.scatter_(2, inp_, 1)\n",
    "    one_hot, _ = one_hot.max(dim=1)\n",
    "    one_hot = one_hot[:, 1:-1]\n",
    "    one_hot[:, 0] = 0\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation metrics\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load ground truth data\n",
    "with open(\"../data/recipe1m_test.pkl\", \"rb\") as f:\n",
    "    test_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build mapping from image filename to ground truth sample\n",
    "imgfile_to_gt = {}\n",
    "for sample in test_data:\n",
    "    for img_filename in sample[\"images\"]:\n",
    "        imgfile_to_gt[img_filename] = sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize accumulators for evaluation\n",
    "predicted_list = []\n",
    "ground_truth_list = []\n",
    "all_ious = []\n",
    "error_types = {\n",
    "    \"tp_i\": 0,\n",
    "    \"fp_i\": 0,\n",
    "    \"fn_i\": 0,\n",
    "    \"tn_i\": 0,\n",
    "    \"tp_all\": 0,\n",
    "    \"fp_all\": 0,\n",
    "    \"fn_all\": 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Collect ground truth and predictions\n",
    "gt_json_list = []\n",
    "pred_json_list = []\n",
    "\n",
    "# Load and preprocess the image\n",
    "for img_file in demo_files:\n",
    "\n",
    "    # Update ground truth sample retrieval\n",
    "    gt_sample = imgfile_to_gt.get(img_file)\n",
    "    if gt_sample is None:\n",
    "        continue\n",
    "\n",
    "    if use_urls:\n",
    "        response = requests.get(img_file)\n",
    "        image = Image.open(BytesIO(response.content))\n",
    "    else:\n",
    "        image_path = os.path.join(image_folder, img_file)\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    transf_list = []\n",
    "    transf_list.append(transforms.Resize(256))\n",
    "    transf_list.append(transforms.CenterCrop(224))\n",
    "    transform = transforms.Compose(transf_list)\n",
    "\n",
    "    image_transf = transform(image)\n",
    "    image_tensor = to_input_transf(image_transf).unsqueeze(0).to(device)\n",
    "\n",
    "    # Ground truth entry\n",
    "    gt_entry = {\n",
    "        \"image_id\": img_file,\n",
    "        \"title\": \" \".join(gt_sample.get(\"title\", [])),\n",
    "        \"ingredients\": gt_sample[\"ingredients\"],\n",
    "        \"instructions\": gt_sample[\"instructions\"],\n",
    "    }\n",
    "\n",
    "    # Predicted recipes for current image\n",
    "    pred_recipes = []\n",
    "    for i in range(numgens):\n",
    "        with torch.no_grad():\n",
    "            outputs = model.sample(\n",
    "                image_tensor,\n",
    "                greedy=greedy[i],\n",
    "                temperature=temperature,\n",
    "                beam=beam[i],\n",
    "                true_ingrs=None,\n",
    "            )\n",
    "\n",
    "        ingr_ids = outputs[\"ingr_ids\"].cpu().numpy()\n",
    "        recipe_ids = outputs[\"recipe_ids\"].cpu().numpy()\n",
    "        outs, valid = prepare_output(recipe_ids[0], ingr_ids[0], ingrs_vocab, vocab)\n",
    "\n",
    "        pred_entry = {\n",
    "            \"title\": outs[\"title\"],\n",
    "            \"ingredients\": outs[\"ingrs\"],\n",
    "            \"instructions\": outs[\"recipe\"],\n",
    "        }\n",
    "        pred_recipes.append(pred_entry)\n",
    "\n",
    "        # Get ground truth ingredients and instructions\n",
    "        gt_ingrs = gt_sample[\"ingredients\"]\n",
    "        gt_instrs = gt_sample[\"instructions\"]\n",
    "\n",
    "        # Convert ground truth ingredients to indices using ingr_vocab\n",
    "        gt_ingr_indices = [\n",
    "            ingrs_vocab.index(ingr) for ingr in gt_ingrs if ingr in ingrs_vocab\n",
    "        ]\n",
    "        if len(gt_ingr_indices) == 0:\n",
    "            continue\n",
    "        pred_ingr_indices = ingr_ids[0]\n",
    "\n",
    "        predicted_instruction = \" \".join(outs[\"recipe\"])\n",
    "        actual_instruction = \" \".join(gt_instrs)\n",
    "        predicted_list.append(predicted_instruction)\n",
    "        ground_truth_list.append(actual_instruction)\n",
    "\n",
    "        pred_tensor = torch.tensor([pred_ingr_indices])\n",
    "        gt_tensor = torch.tensor([gt_ingr_indices])\n",
    "        pred_one_hot = label2onehot(pred_tensor, len(ingrs_vocab) - 1)\n",
    "        gt_one_hot = label2onehot(gt_tensor, len(ingrs_vocab) - 1)\n",
    "\n",
    "        # Print one-hot vectors for debugging:\n",
    "        print(\"Predicted ingredients:\", [ingrs_vocab[i] for i in pred_ingr_indices if i < len(ingrs_vocab)])\n",
    "        print(\"Ground truth ingredients:\", [ingrs_vocab[i] for i in gt_ingr_indices if i < len(ingrs_vocab)])\n",
    "\n",
    "        # Find matching ingredients\n",
    "        matching_indices = set(pred_ingr_indices) & set(gt_ingr_indices)\n",
    "        print(\"Matching ingredients:\", [ingrs_vocab[i] for i in matching_indices if i < len(ingrs_vocab)])\n",
    "\n",
    "        iou = torch.mean(softIoU(pred_one_hot, gt_one_hot)).item()\n",
    "        all_ious.append(iou)\n",
    "        update_error_types(error_types, pred_one_hot, gt_one_hot)\n",
    "\n",
    "    pred_json = {\"image_id\": img_file, \"recipes\": pred_recipes}\n",
    "\n",
    "    # Write ground truth to JSON\n",
    "    with open(f\"Image{img_file}_GroundTruth.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(gt_entry, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    # Write predictions to JSON\n",
    "    with open(f\"Image{img_file}_Predicted.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(pred_json, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics after all samples\n",
    "ret_metrics = {\"accuracy\": [], \"f1\": [], \"jaccard\": [], \"f1_ingredients\": []}\n",
    "compute_metrics(ret_metrics, error_types, [\"f1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate BLEU for all predictions\n",
    "bleu_scores = []\n",
    "smooth = SmoothingFunction().method1\n",
    "for pred, gt in zip(predicted_list, ground_truth_list):\n",
    "    # nltk expects a list of tokens, so split by whitespace\n",
    "    bleu = sentence_bleu([gt.split()], pred.split(), smoothing_function=smooth)\n",
    "    bleu_scores.append(bleu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ROUGE-L for all predictions\n",
    "scorer = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True)\n",
    "rouge_l_scores = []\n",
    "for pred, gt in zip(predicted_list, ground_truth_list):\n",
    "    score = scorer.score(gt, pred)\n",
    "    rouge_l_scores.append(score[\"rougeL\"].fmeasure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print final metrics\n",
    "if len(all_ious) == 0 or len(predicted_list) == 0 or len(ground_truth_list) == 0:\n",
    "    print(\"No valid predictions or ground truths found for metric calculation.\")\n",
    "else:\n",
    "    print(\"Mean Ingredient IoU:\", np.mean(all_ious))\n",
    "    print(\"Ingredient F1:\", np.mean(ret_metrics[\"f1\"]))\n",
    "    print(\"Mean BLEU:\", np.mean(bleu_scores))\n",
    "    print(\"Mean ROUGE-L:\", np.mean(rouge_l_scores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
